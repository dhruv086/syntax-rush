{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22435c7f",
   "metadata": {},
   "source": [
    "# Code Similarity, AI Detection, and Plagiarism Analysis Notebook\n",
    "\n",
    "This notebook provides a comprehensive pipeline for analyzing Python code submissions using advanced machine learning and NLP techniques. It is designed for:\n",
    "\n",
    "- **AI-generated code detection**: Uses GPT-2 language model to estimate code perplexity and pattern analysis to identify AI-generated code.\n",
    "- **Plagiarism detection**: Compares code against common algorithmic patterns and known sources using fast hashing and similarity matching.\n",
    "- **Batch and parallel analysis**: Supports efficient batch processing and parallel execution for large-scale code review.\n",
    "- **Performance metrics**: Includes benchmarking for single and batch analysis, with cache optimization for repeated code checks.\n",
    "- **Device selection**: Automatically detects and utilizes GPU (CUDA) if available for faster inference, otherwise falls back to CPU.\n",
    "- **Model saving**: Demonstrates saving HuggingFace models and tokenizers for production deployment.\n",
    "\n",
    "## Key Components\n",
    "- **OptimizedAIGeneratedCodeDetector**: Singleton class for AI detection using GPT-2, with caching and fast pattern analysis.\n",
    "- **OptimizedPlagiarismDetector**: Detects plagiarism by matching code against pre-computed hashes and known patterns.\n",
    "- **OptimizedCodeChecker**: Integrates AI and plagiarism detection, supports parallel and batch analysis, and provides actionable recommendations.\n",
    "- **Test Suite**: Performance tests for single and batch code analysis, including suspicious, plagiarized, and original code examples.\n",
    "\n",
    "## Usage\n",
    "1. **Check Python environment and device**: Ensure required libraries are installed and GPU is available for optimal performance.\n",
    "2. **Import libraries and classes**: All dependencies are imported and classes are defined for immediate use.\n",
    "3. **Run analysis**: Use the provided test suite or custom code snippets to analyze for AI generation and plagiarism.\n",
    "4. **Save models**: Save trained or pre-trained models for future use or deployment.\n",
    "\n",
    "## Requirements\n",
    "- Python 3.8+\n",
    "- PyTorch\n",
    "- Transformers (HuggingFace)\n",
    "- NumPy, requests, pymongo\n",
    "\n",
    "---\n",
    "\n",
    "> **Note:** This notebook is optimized for speed, scalability, and production-readiness. All detection logic is modular and can be integrated into backend services or automated code review pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f411cc57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e:\\project\\ML\\syntax_env\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211ea68c",
   "metadata": {},
   "source": [
    "### Device Selection and GPU Availability\n",
    "\n",
    "This notebook is optimized to run on systems with a CUDA-enabled GPU for faster code analysis and AI detection. The device check below will automatically detect and display the available GPU. If no GPU is found, the notebook will default to CPU execution.\n",
    "\n",
    "- **Why GPU?** GPU acceleration significantly speeds up model inference and batch processing, making large-scale code review much more efficient.\n",
    "- **Automatic Detection:** The code cell checks for GPU availability and prints the device name if found, or notifies you if only CPU is available.\n",
    "\n",
    "> **Recommendation:** For best performance, run this notebook on a machine with a CUDA-enabled GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "682f73e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available: NVIDIA GeForce GTX 1650\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "gpu_available = torch.cuda.is_available()\n",
    "if gpu_available:\n",
    "    print(f\"GPU available: {torch.cuda.get_device_name(torch.cuda.current_device())}\")\n",
    "else:\n",
    "    print(\"No GPU available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ec7e45",
   "metadata": {},
   "source": [
    "### Project Dependencies\n",
    "\n",
    "This notebook uses a combination of **data processing**, **AI/ML**, **NLP**, and **database** libraries for code analysis and detection tasks.\n",
    "\n",
    "**Core Libraries:**\n",
    "- `numpy` – Numerical computations and array operations  \n",
    "- `torch` – PyTorch for deep learning and model inference  \n",
    "- `transformers` – Hugging Face models and tokenizers (`AutoTokenizer`, `AutoModel`, `GPT2LMHeadModel`, `GPT2Tokenizer`)  \n",
    "\n",
    "**Code Analysis & Processing:**\n",
    "- `re` – Regular expressions for pattern matching  \n",
    "- `ast` – Abstract Syntax Tree parsing for code inspection  \n",
    "- `hashlib` – Generating hashes for code fingerprinting  \n",
    "- `difflib` – Sequence matching for similarity detection  \n",
    "\n",
    "**Performance & Utilities:**\n",
    "- `functools.lru_cache` – Caching for faster repeated computations  \n",
    "- `concurrent.futures.ThreadPoolExecutor` – Parallel execution  \n",
    "- `warnings` – Suppressing unnecessary warnings   \n",
    "\n",
    "\n",
    "> ⚡ **Tip:** Importing these libraries upfront ensures smooth execution for code analysis, AI detection, and database operations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03a58114",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import re\n",
    "import ast\n",
    "import hashlib\n",
    "import difflib\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from functools import lru_cache\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from typing import Dict,List,Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8722481c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## AI-Generated Code Detection Logic\n",
    "\n",
    "This section explains the logic and methodology behind detecting AI-generated code submissions:\n",
    "\n",
    "- **Perplexity Analysis**: Utilizes the GPT-2 language model to calculate the perplexity of code snippets. Lower perplexity values often indicate AI-generated code due to the model's familiarity with such patterns.\n",
    "- **Pattern Recognition**: Analyzes code structure, variable naming conventions, comment ratios, and indentation consistency to identify characteristics typical of AI-generated code.\n",
    "- **Caching for Speed**: Implements LRU caching to accelerate repeated analysis and improve scalability for large datasets.\n",
    "- **Singleton Model Loading**: Ensures models are loaded only once per session, reducing memory usage and initialization time.\n",
    "- **Batch and Parallel Processing**: Supports efficient batch analysis and parallel execution for rapid review of multiple code samples.\n",
    "\n",
    "> This modular AI detection logic is designed for integration into automated code review systems, online judges, and educational platforms to help maintain code authenticity and integrity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "517336b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedAIGeneratedCodeDetector:\n",
    "    \"\"\"Optimized AI detection with caching and faster inference\"\"\"\n",
    "    \n",
    "    _instance = None\n",
    "    \n",
    "    def __new__(cls):\n",
    "        \"\"\"Singleton pattern - load models only once\"\"\"\n",
    "        if cls._instance is None:\n",
    "            cls._instance = super().__new__(cls)\n",
    "            cls._instance._initialized = False\n",
    "        return cls._instance\n",
    "    \n",
    "    def __init__(self):\n",
    "        if self._initialized:\n",
    "            return\n",
    "            \n",
    "        # Load models once and keep in memory\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"GPT-2 model loaded on {self.device}.\")\n",
    "        \n",
    "        self.gpt2_model = GPT2LMHeadModel.from_pretrained('gpt2').to(self.device)\n",
    "        self.gpt2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "        self.gpt2_tokenizer.pad_token = self.gpt2_tokenizer.eos_token\n",
    "        self.gpt2_model.eval() # model in eval mode for better inference.\n",
    "        \n",
    "        self._initialized = True\n",
    "        print(\"GPT-2 model loaded succesfully!.\")\n",
    "    \n",
    "\n",
    "    def calculate_perplexity(self,code):\n",
    "        \"\"\"Calculate perplexity score of given code.\n",
    "           Score : Low:- AI likely, High:- Human likely.\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            code_lines=[line.strip() for line in code.split('\\n') if line.strip()]\n",
    "            code_text=' '.join(code_lines)\n",
    "\n",
    "            encodings=self.gpt2_tokenizer(\n",
    "                code_text,\n",
    "                return_tensors='pt',\n",
    "                trucation=True,\n",
    "                max_length=512\n",
    "            ).to(self.device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs=self.gpt2_model(**encodings,labels=encodings['inputs_ids'])\n",
    "                perplexity=torch.exp(outputs.loss)\n",
    "\n",
    "            return float(perplexity)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Eroor calculation failed!!! : {e}\")\n",
    "            return 0.0\n",
    "        \n",
    "\n",
    "    def extract_ast_features(self, code):\n",
    "        \"\"\"Extract AST structural features\"\"\"\n",
    "        features = {}\n",
    "        \n",
    "        try:\n",
    "            tree = ast.parse(code)\n",
    "            \n",
    "            node_types = [type(node).__name__ for node in ast.walk(tree)]\n",
    "            total_nodes = len(node_types)\n",
    "            features['total_nodes'] = total_nodes\n",
    "            features['unique_node_ratio'] = len(set(node_types)) / max(total_nodes, 1)\n",
    "            \n",
    "            def get_depths(node, depth=0):\n",
    "                depths = [depth]\n",
    "                for child in ast.iter_child_nodes(node):\n",
    "                    depths.extend(get_depths(child, depth + 1))\n",
    "                return depths\n",
    "            \n",
    "            depths = get_depths(tree)\n",
    "            features['max_depth'] = max(depths) if depths else 0\n",
    "            features['depth_variance'] = np.var(depths) if len(depths) > 1 else 0\n",
    "            \n",
    "            num_conditions = sum(1 for _ in ast.walk(tree) if isinstance(_, ast.If))\n",
    "            num_loops = sum(1 for _ in ast.walk(tree) if isinstance(_, (ast.For, ast.While)))\n",
    "            num_try = sum(1 for _ in ast.walk(tree) if isinstance(_, ast.Try))\n",
    "            \n",
    "            features['num_conditions'] = num_conditions\n",
    "            features['num_loops'] = num_loops\n",
    "            features['cyclomatic_complexity'] = 1 + num_conditions + num_loops\n",
    "            \n",
    "            features['is_perfectly_balanced'] = (\n",
    "                features['depth_variance'] < 1.0 and \n",
    "                features['cyclomatic_complexity'] < 4\n",
    "            )\n",
    "            features['has_error_handling'] = num_try > 0\n",
    "            features['parse_success'] = True\n",
    "            \n",
    "        except:\n",
    "            features['parse_success'] = False\n",
    "            features['parse_error'] = True\n",
    "        \n",
    "        return features    \n",
    "\n",
    "    def analyze_code_patterns(self, code):\n",
    "        \"\"\"Analyze code patterns (naming, comments, style)\"\"\"\n",
    "        features = {}\n",
    "        \n",
    "        lines = code.split('\\n')\n",
    "        total_lines = len(lines)\n",
    "        \n",
    "        comment_lines = sum(1 for line in lines if line.strip().startswith('#'))\n",
    "        docstring_count = len(re.findall(r'\"\"\"[\\s\\S]*?\"\"\"|\\'\\'\\'[\\s\\S]*?\\'\\'\\'', code))\n",
    "        features['comment_ratio'] = comment_lines / max(total_lines, 1)\n",
    "        features['has_docstrings'] = docstring_count > 0\n",
    "        \n",
    "        try:\n",
    "            var_pattern = r'\\b[a-zA-Z_][a-zA-Z0-9_]*\\b'\n",
    "            var_names = re.findall(var_pattern, code)\n",
    "            keywords = {'def', 'class', 'if', 'else', 'for', 'while', 'return', 'import'}\n",
    "            var_names = [v for v in var_names if v not in keywords]\n",
    "            \n",
    "            if var_names:\n",
    "                avg_name_length = np.mean([len(name) for name in var_names])\n",
    "                long_names = sum(1 for name in var_names if len(name) >= 10)\n",
    "                \n",
    "                features['avg_var_name_length'] = avg_name_length\n",
    "                features['long_name_ratio'] = long_names / len(var_names)\n",
    "            else:\n",
    "                features['avg_var_name_length'] = 0\n",
    "                features['long_name_ratio'] = 0\n",
    "        except:\n",
    "            features['avg_var_name_length'] = 0\n",
    "            features['long_name_ratio'] = 0\n",
    "        \n",
    "        indents = [len(line) - len(line.lstrip()) for line in lines if line.strip()]\n",
    "        if indents:\n",
    "            features['indent_consistency'] = 1 - (np.std(indents) / (np.mean(indents) + 1))\n",
    "        else:\n",
    "            features['indent_consistency'] = 0\n",
    "        \n",
    "        return features\n",
    "\n",
    "\n",
    "    def detect_ai_generated(self, code):\n",
    "        \"\"\"\n",
    "        Main AI detection method\n",
    "        Combines perplexity (60%), AST (25%), patterns (15%)\n",
    "        \"\"\"\n",
    "        \n",
    "        perplexity = self.calculate_perplexity(code)\n",
    "        ast_features = self.extract_ast_features(code)\n",
    "        pattern_features = self.analyze_code_patterns(code)\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"AI DETECTION ANALYSIS\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        # PERPLEXITY SCORING (60% weight)\n",
    "        print(f\"\\n1. PERPLEXITY ANALYSIS\")\n",
    "        print(f\"   Value: {perplexity:.2f}\")\n",
    "        \n",
    "        if perplexity < 10:\n",
    "            perplexity_score = 0.60\n",
    "            perplexity_level = \"CRITICAL\"\n",
    "            interpretation = \"EXTREMELY LOW - Very strong AI signature\"\n",
    "        elif perplexity < 20:\n",
    "            perplexity_score = 0.50\n",
    "            perplexity_level = \"HIGH\"\n",
    "            interpretation = \"Very low - Strong AI indicator\"\n",
    "        elif perplexity < 35:\n",
    "            perplexity_score = 0.35\n",
    "            perplexity_level = \"MODERATE-HIGH\"\n",
    "            interpretation = \"Low - Significant AI suspicion\"\n",
    "        elif perplexity < 60:\n",
    "            perplexity_score = 0.20\n",
    "            perplexity_level = \"MODERATE\"\n",
    "            interpretation = \"Moderate - Some AI patterns\"\n",
    "        elif perplexity < 100:\n",
    "            perplexity_score = 0.10\n",
    "            perplexity_level = \"LOW\"\n",
    "            interpretation = \"Higher - Leaning human\"\n",
    "        else:\n",
    "            perplexity_score = 0.0\n",
    "            perplexity_level = \"NONE\"\n",
    "            interpretation = \"High - Human-like variability\"\n",
    "        \n",
    "        print(f\"   Level: {perplexity_level}\")\n",
    "        print(f\"   {interpretation}\")\n",
    "        print(f\"   Score: {perplexity_score:.3f} / 0.600\")\n",
    "        \n",
    "        # AST SCORING (25% weight)\n",
    "        print(f\"\\n2. AST STRUCTURE ANALYSIS\")\n",
    "        print(f\"   Balanced: {ast_features.get('is_perfectly_balanced', False)}\")\n",
    "        print(f\"   Depth Variance: {ast_features.get('depth_variance', 0):.2f}\")\n",
    "        print(f\"   Complexity: {ast_features.get('cyclomatic_complexity', 0)}\")\n",
    "        \n",
    "        ast_score = 0.0\n",
    "        \n",
    "        if not ast_features.get('parse_error', False):\n",
    "            if ast_features.get('is_perfectly_balanced', False):\n",
    "                ast_score += 0.15\n",
    "                print(f\"   Perfect balance detected (+0.15)\")\n",
    "            \n",
    "            if ast_features.get('depth_variance', 10) < 1.5:\n",
    "                ast_score += 0.10\n",
    "                print(f\"   Low depth variance (+0.10)\")\n",
    "            \n",
    "            if ast_features.get('unique_node_ratio', 1.0) < 0.35:\n",
    "                ast_score += 0.05\n",
    "        \n",
    "        ast_score = min(ast_score, 0.25)\n",
    "        print(f\"   Score: {ast_score:.3f} / 0.250\")\n",
    "        \n",
    "        # PATTERN SCORING (15% weight)\n",
    "        print(f\"\\n3. PATTERN ANALYSIS\")\n",
    "        print(f\"   Avg Variable Length: {pattern_features.get('avg_var_name_length', 0):.1f}\")\n",
    "        print(f\"   Comment Ratio: {pattern_features.get('comment_ratio', 0):.2f}\")\n",
    "        print(f\"   Indent Consistency: {pattern_features.get('indent_consistency', 0):.2f}\")\n",
    "        \n",
    "        pattern_score = 0.0\n",
    "        \n",
    "        if pattern_features.get('avg_var_name_length', 0) > 15:\n",
    "            pattern_score += 0.08\n",
    "            print(f\"   Very long variable names (+0.08)\")\n",
    "        elif pattern_features.get('avg_var_name_length', 0) > 10:\n",
    "            pattern_score += 0.04\n",
    "        \n",
    "        if pattern_features.get('has_docstrings', False) and pattern_features.get('comment_ratio', 0) > 0.3:\n",
    "            pattern_score += 0.07\n",
    "            print(f\"   Excessive documentation (+0.07)\")\n",
    "        \n",
    "        if pattern_features.get('indent_consistency', 0) > 0.98:\n",
    "            pattern_score += 0.05\n",
    "        \n",
    "        pattern_score = min(pattern_score, 0.15)\n",
    "        print(f\"   Score: {pattern_score:.3f} / 0.150\")\n",
    "        \n",
    "        # FINAL SCORING\n",
    "        total_score = perplexity_score + ast_score + pattern_score\n",
    "        ai_probability = min(total_score, 1.0)\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"TOTAL AI PROBABILITY: {ai_probability:.3f} ({ai_probability*100:.1f}%)\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        if ai_probability >= 0.65:\n",
    "            verdict = \"HIGH CONFIDENCE: Code is very likely AI-generated\"\n",
    "            risk_level = \"HIGH\"\n",
    "        elif ai_probability >= 0.45:\n",
    "            verdict = \"MODERATE CONFIDENCE: Code shows significant AI patterns\"\n",
    "            risk_level = \"MEDIUM\"\n",
    "        elif ai_probability >= 0.30:\n",
    "            verdict = \"LOW CONFIDENCE: Some AI-like patterns detected\"\n",
    "            risk_level = \"LOW\"\n",
    "        else:\n",
    "            verdict = \"CLEAN: Code appears human-written\"\n",
    "            risk_level = \"NONE\"\n",
    "        \n",
    "        conflict_detected = False\n",
    "        if perplexity < 15 and ast_score < 0.10:\n",
    "            conflict_detected = True\n",
    "            verdict += \" [CONFLICT: Low perplexity but human-like structure]\"\n",
    "            print(f\"\\nCONFLICT DETECTED:\")\n",
    "            print(f\"  Perplexity signals strong AI ({perplexity:.2f})\")\n",
    "            print(f\"  But AST structure appears human-like\")\n",
    "        \n",
    "        return {\n",
    "            'ai_probability': round(ai_probability, 3),\n",
    "            'is_ai_generated': ai_probability >= 0.45,\n",
    "            'risk_level': risk_level,\n",
    "            'verdict': verdict,\n",
    "            'conflict_detected': conflict_detected,\n",
    "            'score_breakdown': {\n",
    "                'perplexity_score': round(perplexity_score, 3),\n",
    "                'ast_score': round(ast_score, 3),\n",
    "                'pattern_score': round(pattern_score, 3)\n",
    "            },\n",
    "            'metrics': {\n",
    "                'perplexity': round(perplexity, 2),\n",
    "                'perplexity_level': perplexity_level,\n",
    "                'ast_features': ast_features,\n",
    "                'pattern_features': pattern_features\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae39de7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Plagiarism Detection Logic\n",
    "\n",
    "This section details the approach used for detecting plagiarism in code submissions:\n",
    "\n",
    "- **Common Algorithm Patterns**: The detector checks submitted code against a set of well-known algorithmic patterns (e.g., quicksort, binary search, bubble sort, merge sort, recursive Fibonacci).\n",
    "- **Hash-Based Matching**: Each pattern is pre-processed and stored as a hash for fast exact matching. If a code snippet matches a known pattern hash, it is flagged as an exact match.\n",
    "- **Similarity Analysis**: For non-exact matches, the detector uses sequence similarity to compare code structure and logic, flagging submissions with high similarity scores.\n",
    "- **Source Attribution**: When plagiarism is detected, the system reports the sources (e.g., StackOverflow, GitHub, LeetCode) where the matching pattern is commonly found.\n",
    "- **Performance**: The logic is optimized for speed using caching and early termination, making it suitable for large-scale automated code review.\n",
    "\n",
    "> This modular approach allows for easy extension with new patterns and sources, and can be integrated into backend services for real-time plagiarism detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0efcda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedCodeNormalizer:\n",
    "    \"\"\"Code normalization for plagiarism detection\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def normalize_code(code: str, level: str = 'medium') -> str:\n",
    "        \"\"\"\n",
    "        Normalize code at different levels\n",
    "        \n",
    "        Args:\n",
    "            code: Source code\n",
    "            level: 'light', 'medium', 'aggressive'\n",
    "        \"\"\"\n",
    "        if level == 'light':\n",
    "            return OptimizedCodeNormalizer._light_normalize(code)\n",
    "        elif level == 'medium':\n",
    "            return OptimizedCodeNormalizer._medium_normalize(code)\n",
    "        else:\n",
    "            return OptimizedCodeNormalizer._aggressive_normalize(code)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _light_normalize(code: str) -> str:\n",
    "        \"\"\"Remove comments and whitespace\"\"\"\n",
    "        code = re.sub(r'#.*$', '', code, flags=re.MULTILINE)\n",
    "        code = re.sub(r'\"\"\"[\\s\\S]*?\"\"\"', '', code)\n",
    "        code = re.sub(r\"'''[\\s\\S]*?'''\", '', code)\n",
    "        code = re.sub(r'\\s+', ' ', code).strip()\n",
    "        return code\n",
    "    \n",
    "    @staticmethod\n",
    "    def _medium_normalize(code: str) -> str:\n",
    "        \"\"\"Normalize and rename variables\"\"\"\n",
    "        code = OptimizedCodeNormalizer._light_normalize(code)\n",
    "        \n",
    "        try:\n",
    "            tree = ast.parse(code)\n",
    "            \n",
    "            class VariableRenamer(ast.NodeTransformer):\n",
    "                def __init__(self):\n",
    "                    self.var_map = {}\n",
    "                    self.counter = 0\n",
    "                    self.builtins = {'print', 'len', 'range', 'str', 'int', 'float', \n",
    "                                   'list', 'dict', 'set', 'sum', 'max', 'min', 'input'}\n",
    "                \n",
    "                def visit_Name(self, node):\n",
    "                    if node.id not in self.builtins:\n",
    "                        if node.id not in self.var_map:\n",
    "                            self.var_map[node.id] = f'v{self.counter}'\n",
    "                            self.counter += 1\n",
    "                        node.id = self.var_map[node.id]\n",
    "                    return node\n",
    "                \n",
    "                def visit_FunctionDef(self, node):\n",
    "                    if node.name not in self.var_map:\n",
    "                        self.var_map[node.name] = f'f{self.counter}'\n",
    "                        self.counter += 1\n",
    "                    node.name = self.var_map[node.name]\n",
    "                    self.generic_visit(node)\n",
    "                    return node\n",
    "            \n",
    "            renamer = VariableRenamer()\n",
    "            tree = renamer.visit(tree)\n",
    "            code = ast.unparse(tree)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        return code\n",
    "    \n",
    "    @staticmethod\n",
    "    def _aggressive_normalize(code: str) -> str:\n",
    "        \"\"\"Full normalization\"\"\"\n",
    "        code = OptimizedCodeNormalizer._medium_normalize(code)\n",
    "        code = re.sub(r'\\s+', '', code)\n",
    "        code = code.lower()\n",
    "        return code\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "713f5275",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedPlagiarismDetector:\n",
    "    \n",
    "    \"\"\"Optimized plagiarism detection with faster matching\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.normalizer=OptimizedCodeNormalizer()\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.pattern_hashes = {}\n",
    "        self.patterns_by_hash = {}\n",
    "        self.pattern_database={}\n",
    "        self._load_common_snippets()\n",
    "        \n",
    "\n",
    "    def _load_common_snippets(self):\n",
    "\n",
    "        \"\"\"L=Common algorithms patterns\"\"\"\n",
    "\n",
    "        patterns = {\n",
    "            'quicksort_basic': {\n",
    "                'code': 'def quicksort(arr): if len(arr) <= 1: return arr',\n",
    "                'category':'sorting',\n",
    "                'sources': ['stackoverflow', 'github']\n",
    "            },\n",
    "            'fibonacci_recursive': {\n",
    "                'code': 'def fibonacci(n): if n <= 1: return n return fibonacci(n-1) + fibonacci(n-2)',\n",
    "                'category': 'dynamic_programming',\n",
    "                'sources': ['common_algorithm']\n",
    "            },\n",
    "            'binary_search': {\n",
    "                'code': 'def binary_search(arr, target): left = 0 right = len(arr) - 1',\n",
    "                'category': 'searching',\n",
    "                'sources': ['leetcode', 'github']\n",
    "            },\n",
    "            'bubble_sort': {\n",
    "                'code': 'def bubble_sort(arr): for i in range(len(arr)): for j in range(len(arr)-i-1):',\n",
    "                'category':'sorting',\n",
    "                'sources': ['stackoverflow']\n",
    "            },\n",
    "            'merge_sort': {\n",
    "                'code': 'def merge_sort(arr): if len(arr) > 1: mid = len(arr) // 2',\n",
    "                'category':'sorting',\n",
    "                'sources': ['github', 'common_algorithm']\n",
    "            },\n",
    "            'bfs': {\n",
    "                'code': 'def bfs(graph, start): visited = set() queue = [start] while queue:',\n",
    "                'category': 'graph',\n",
    "                'sources': ['stackoverflow']\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Pre-compute all hashes\n",
    "        for name, info in patterns.items():\n",
    "            normalized = self.normalizer.normalize_code(info['code'], level='medium')\n",
    "            pattern_hash = hashlib.md5(normalized.encode()).hexdigest()\n",
    "            self.pattern_hashes[name] = pattern_hash\n",
    "            self.patterns_by_hash[pattern_hash] = {\n",
    "                'name': name,\n",
    "                'code':info['code'],\n",
    "                'normalized':normalized,\n",
    "                'category': info['category'],\n",
    "                'sources': info['sources'],\n",
    "            }\n",
    "            self.pattern_database[name]=info\n",
    "    \n",
    "    @lru_cache(maxsize=2000)\n",
    "    def _cached_normalize(self, code:str,level:str)->str:\n",
    "        \"\"\"Cached normalization \"\"\"\n",
    "        \n",
    "        return self.normalizer.normalize_code(code,level)\n",
    "    \n",
    "    def check_pattern_databse(self, code:str)->List[dict]:\n",
    "        \n",
    "        matches=[]\n",
    "\n",
    "        for level in ['medium', 'aggressive']:\n",
    "            normalized_code = self._cached_normalize(code, level)\n",
    "            code_hash = hashlib.md5(normalized_code.encode()).hexdigest()\n",
    "            \n",
    "            if code_hash in self.patterns_by_hash:\n",
    "                pattern_info = self.patterns_by_hash[code_hash]\n",
    "                matches.append({\n",
    "                    'pattern_name': pattern_info['name'],\n",
    "                    'category': pattern_info['category'],\n",
    "                    'similarity': 1.0,\n",
    "                    'sources': pattern_info['sources'],\n",
    "                    'match_type': 'exact',\n",
    "                    'normalization_level': level\n",
    "                })\n",
    "                continue\n",
    "            \n",
    "            for pattern_hash, pattern_info in self.patterns_by_hash.items():\n",
    "                pattern_normalized = pattern_info['normalized']\n",
    "                \n",
    "                len_ratio = len(normalized_code) / max(len(pattern_normalized), 1)\n",
    "                if len_ratio < 0.5 or len_ratio > 2.0:\n",
    "                    continue\n",
    "                \n",
    "                similarity = difflib.SequenceMatcher(\n",
    "                    None, \n",
    "                    normalized_code, \n",
    "                    pattern_normalized\n",
    "                ).ratio()\n",
    "                \n",
    "                if similarity > 0.75:\n",
    "                    matches.append({\n",
    "                        'pattern_name': pattern_info['name'],\n",
    "                        'category': pattern_info['category'],\n",
    "                        'similarity': similarity,\n",
    "                        'sources': pattern_info['sources'],\n",
    "                        'match_type': 'similar',\n",
    "                        'normalization_level': level\n",
    "                    })\n",
    "        \n",
    "        seen = set()\n",
    "        unique_matches = []\n",
    "        for match in matches:\n",
    "            key = (match['pattern_name'], match['match_type'])\n",
    "            if key not in seen:\n",
    "                seen.add(key)\n",
    "                unique_matches.append(match)\n",
    "        \n",
    "        return unique_matches\n",
    "        \n",
    "    \n",
    "    def compare_submissions(self, code1: str, code2: str) -> Dict:\n",
    "        \"\"\"Compare two submissions\"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        for level in ['light', 'medium', 'aggressive']:\n",
    "            norm1 = self._cached_normalize(code1, level)\n",
    "            norm2 = self._cached_normalize(code2, level)\n",
    "            \n",
    "            similarity = difflib.SequenceMatcher(None, norm1, norm2).ratio()\n",
    "            results[f'{level}_similarity'] = similarity\n",
    "        \n",
    "        try:\n",
    "            tree1 = ast.parse(code1)\n",
    "            tree2 = ast.parse(code2)\n",
    "            \n",
    "            dump1 = ast.dump(tree1)\n",
    "            dump2 = ast.dump(tree2)\n",
    "            \n",
    "            structural_similarity = difflib.SequenceMatcher(None, dump1, dump2).ratio()\n",
    "            results['structural_similarity'] = structural_similarity\n",
    "        except:\n",
    "            results['structural_similarity'] = 0.0\n",
    "        \n",
    "        results['max_similarity'] = max(\n",
    "            results.get('light_similarity', 0),\n",
    "            results.get('medium_similarity', 0),\n",
    "            results.get('aggressive_similarity', 0)\n",
    "        )\n",
    "        \n",
    "        results['is_similar'] = results['max_similarity'] > 0.85\n",
    "        \n",
    "        return results\n",
    "\n",
    "\n",
    "\n",
    "    def detect_plagiarism(self, code: str, submission_database: Optional[List[Dict]] = None) -> Dict:\n",
    "        \"\"\"\n",
    "        Main plagiarism detection method\n",
    "        \n",
    "        Args:\n",
    "            code: Code to analyze\n",
    "            submission_database: Optional list of previous submissions\n",
    "        \"\"\"\n",
    "        results = {\n",
    "            'is_plagiarized': False,\n",
    "            'confidence': 0.0,\n",
    "            'pattern_matches': [],\n",
    "            'submission_matches': [],\n",
    "            'sources_found': [],\n",
    "            'risk_level': 'NONE'\n",
    "        }\n",
    "        \n",
    "        pattern_matches = self.check_pattern_database(code)\n",
    "        results['pattern_matches'] = pattern_matches\n",
    "        \n",
    "        if pattern_matches:\n",
    "            max_pattern_similarity = max(m['similarity'] for m in pattern_matches)\n",
    "            results['sources_found'] = list(set(\n",
    "                source for match in pattern_matches for source in match['sources']\n",
    "            ))\n",
    "            \n",
    "            if max_pattern_similarity >= 0.95:\n",
    "                results['is_plagiarized'] = True\n",
    "                results['confidence'] = max_pattern_similarity\n",
    "                results['risk_level'] = 'HIGH'\n",
    "                results['reason'] = \"Exact copy of common pattern\"\n",
    "            elif max_pattern_similarity >= 0.85:\n",
    "                results['is_plagiarized'] = True\n",
    "                results['confidence'] = max_pattern_similarity\n",
    "                results['risk_level'] = 'MEDIUM'\n",
    "                results['reason'] = \"Very similar to common pattern\"\n",
    "        \n",
    "        if submission_database:\n",
    "            for submission in submission_database:\n",
    "                comparison = self.compare_submissions(code, submission['code'])\n",
    "                \n",
    "                if comparison['max_similarity'] > 0.85:\n",
    "                    results['submission_matches'].append({\n",
    "                        'user_id': submission.get('user_id', 'unknown'),\n",
    "                        'timestamp': submission.get('timestamp', 'unknown'),\n",
    "                        'similarity': comparison['max_similarity'],\n",
    "                        'details': comparison\n",
    "                    })\n",
    "                    \n",
    "                    if comparison['max_similarity'] > results['confidence']:\n",
    "                        results['is_plagiarized'] = True\n",
    "                        results['confidence'] = comparison['max_similarity']\n",
    "                        results['risk_level'] = 'HIGH' if comparison['max_similarity'] > 0.95 else 'MEDIUM'\n",
    "                        results['reason'] = \"Matches another submission\"\n",
    "        \n",
    "        if not results['is_plagiarized'] and results['pattern_matches']:\n",
    "            avg_similarity = np.mean([m['similarity'] for m in pattern_matches])\n",
    "            if avg_similarity > 0.6:\n",
    "                results['risk_level'] = 'LOW'\n",
    "                results['reason'] = \"Uses common algorithm patterns (acceptable)\"\n",
    "        \n",
    "        return results\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e4fa10",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Code Analysis and Recommendation Engine\n",
    "\n",
    "This section introduces the integrated code analysis engine, which combines AI-generated code detection and plagiarism detection to provide a comprehensive assessment of code submissions:\n",
    "\n",
    "- **Parallel Processing**: The engine runs AI and plagiarism checks in parallel for faster results, making it suitable for batch analysis and large datasets.\n",
    "- **Suspiciousness Scoring**: Each code snippet is evaluated for signs of AI generation and plagiarism, with an overall suspiciousness score and detailed breakdown.\n",
    "- **Actionable Recommendations**: Based on the analysis, the engine provides clear recommendations, such as flagging high-risk submissions, suggesting manual review, or confirming clean code.\n",
    "- **Batch Support**: Multiple code samples can be analyzed simultaneously, with results aggregated for efficient review.\n",
    "- **Cache Management**: Built-in cache clearing methods help manage memory and maintain performance during repeated or large-scale analysis.\n",
    "\n",
    "> This modular recommendation engine can be integrated into automated code review systems, online judges, or educational platforms to enhance code integrity and fairness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "371bfcdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class OptimizedCodeChecker:\n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "        self.ai_detector = OptimizedAIGeneratedCodeDetector()\n",
    "        self.plagiarism_detector = OptimizedPlagiarismDetector()\n",
    "        self.executor=ThreadPoolExecutor(max_workers=2)\n",
    "    \n",
    "    def analyze_code(self, code:str, submission_database:Optional[List[Dict]]=None,parallel:bool=True)->Dict:\n",
    "        \n",
    "        if parallel:\n",
    "            \n",
    "            future_ai = self.executor.submit(self.ai_detector.detect_ai_generated, code)\n",
    "            future_plag = self.executor.submit(self.plagiarism_detector.detect_plagiarism, code,submission_database)\n",
    "            \n",
    "            ai_result = future_ai.result()\n",
    "            plag_result = future_plag.result()\n",
    "        else:\n",
    "            ai_result = self.ai_detector.detect_ai_generated(code)\n",
    "            plag_result = self.plagiarism_detector.detect_plagiarism(code)\n",
    "        \n",
    "        overall_suspicious = (\n",
    "            ai_result['is_ai_generated'] or \n",
    "            plag_result['is_plagiarized']\n",
    "        )\n",
    "\n",
    "        risk_levels={'NONE': 0, 'LOW': 1, 'MEDIUM': 2, 'HIGH': 3}\n",
    "        ai_risk = risk_levels.get(ai_result['risk_level'], 0)\n",
    "        plag_risk = risk_levels.get(plag_result['risk_level'], 0)\n",
    "        \n",
    "        overall_risk_value = max(ai_risk, plag_risk)\n",
    "        overall_risk = [k for k, v in risk_levels.items() if v == overall_risk_value][0]\n",
    "\n",
    "\n",
    "        return{\n",
    "            'overall_suspicious': overall_suspicious,\n",
    "            'overall_risk_level':overall_risk,\n",
    "            'ai_detection': ai_result,\n",
    "            'plagiarism_detection': plag_result,\n",
    "            'recommendation': self._generate_recommendation(ai_result, plag_result),\n",
    "            'action':self._determine_action(ai_result,plag_result)\n",
    "        }\n",
    "    \n",
    "    def analyze_batch(self, codes:List[str])->List[Dict]:\n",
    "        \n",
    "        results = []\n",
    "        with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "            futures = [executor.submit(self.analyze_code, code,None, False) for code in codes]\n",
    "            results = [future.result() for future in futures]\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _get_recommendation(self, ai_result:Dict, plag_result:Dict)->str:\n",
    "        \"\"\"Fast recommendation generation\"\"\"\n",
    "        \n",
    "        if ai_result['is_ai_generated'] and plag_result['is_plagiarized']:\n",
    "            return (f\"CRITICAL: Code shows both AI generation \"\n",
    "                   f\"(confidence: {ai_result['ai_probability']:.1%}) \"\n",
    "                   f\"and plagiarism (similarity: {plag_result['confidence']:.1%})\")\n",
    "        \n",
    "        elif ai_result['is_ai_generated']:\n",
    "            return (f\"AI DETECTED: Code likely AI-generated \"\n",
    "                   f\"(confidence: {ai_result['ai_probability']:.1%})\")\n",
    "        \n",
    "        elif plag_result['is_plagiarized']:\n",
    "            sources = plag_result.get('sources_found', [])\n",
    "            if sources:\n",
    "                source_str = ', '.join(sources)\n",
    "                return f\"PLAGIARISM DETECTED: Matches known sources ({source_str})\"\n",
    "            else:\n",
    "                return \"PLAGIARISM DETECTED: Matches other submissions\"\n",
    "        \n",
    "        elif ai_result['ai_probability'] > 0.4:\n",
    "            return \"MODERATE RISK: Some AI-like patterns detected\"\n",
    "        \n",
    "        elif plag_result['risk_level'] == 'LOW':\n",
    "            return \"ACCEPTABLE: Uses common algorithm patterns\"\n",
    "        \n",
    "        else:\n",
    "            return \"CLEAN: No significant issues detected\"\n",
    "    \n",
    "    def _determine_action(self, ai_result: Dict, plag_result: Dict) -> str:\n",
    "        \"\"\"Determine action\"\"\"\n",
    "        \n",
    "        if ai_result['is_ai_generated'] and plag_result['is_plagiarized']:\n",
    "            return \"BLOCK_AND_REPORT\"\n",
    "        elif plag_result['is_plagiarized'] and plag_result['risk_level'] == 'HIGH':\n",
    "            return \"INVESTIGATE\"\n",
    "        elif ai_result['is_ai_generated'] and ai_result['risk_level'] == 'HIGH':\n",
    "            return \"FLAG_FOR_REVIEW\"\n",
    "        elif ai_result['ai_probability'] > 0.4 or plag_result['risk_level'] != 'NONE':\n",
    "            return \"MONITOR\"\n",
    "        else:\n",
    "            return \"ACCEPT\"\n",
    "        \n",
    "\n",
    "    def clear_cache(self):\n",
    "        \"\"\"Clear LRU caches to free memory\"\"\"\n",
    "        if hasattr(self.plagiarism_detector, '_cached_normalize'):\n",
    "            self.plagiarism_detector._cached_normalize.cache_clear()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c94f412",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Test Cases and Performance Metrics\n",
    "\n",
    "This section provides a suite of test cases to validate the code analysis engine and benchmark its performance:\n",
    "\n",
    "- **Test Coverage**: Includes examples of suspicious AI-like code, common algorithms (potential plagiarism), and original code to demonstrate detection capabilities.\n",
    "- **Single and Batch Analysis**: Measures the time taken for individual and batch code analysis, highlighting the speedup from parallel processing.\n",
    "- **Performance Reporting**: Outputs suspiciousness, AI probability, plagiarism status, and recommendations for each test case, along with timing metrics.\n",
    "- **Cache Efficiency**: (Commented) Optionally tests cache performance for repeated analysis, showing the benefits of caching in large-scale scenarios.\n",
    "\n",
    "> Use these tests to ensure the reliability and efficiency of the detection pipeline before deploying in production or integrating with automated review systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d9133888",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'trucation': True} not recognized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "COMPREHENSIVE CODE ANALYSIS TEST\n",
      "======================================================================\n",
      "Eroor calculation failed!!! : 'inputs_ids'\n",
      "\n",
      "======================================================================\n",
      "AI DETECTION ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "1. PERPLEXITY ANALYSIS\n",
      "   Value: 0.00\n",
      "   Level: CRITICAL\n",
      "   EXTREMELY LOW - Very strong AI signature\n",
      "   Score: 0.600 / 0.600\n",
      "\n",
      "2. AST STRUCTURE ANALYSIS\n",
      "   Balanced: False\n",
      "   Depth Variance: 4.81\n",
      "   Complexity: 4\n",
      "   Score: 0.050 / 0.250\n",
      "\n",
      "3. PATTERN ANALYSIS\n",
      "   Avg Variable Length: 2.7\n",
      "   Comment Ratio: 0.00\n",
      "   Indent Consistency: 0.36\n",
      "   Score: 0.000 / 0.150\n",
      "\n",
      "======================================================================\n",
      "TOTAL AI PROBABILITY: 0.650 (65.0%)\n",
      "======================================================================\n",
      "\n",
      "CONFLICT DETECTED:\n",
      "  Perplexity signals strong AI (0.00)\n",
      "  But AST structure appears human-like\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'OptimizedPlagiarismDetector' object has no attribute 'check_pattern_database'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCOMPREHENSIVE CODE ANALYSIS TEST\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m70\u001b[39m)\n\u001b[1;32m---> 17\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mchecker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43manalyze_code\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_code\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mOVERALL ASSESSMENT:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Suspicious: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moverall_suspicious\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[13], line 17\u001b[0m, in \u001b[0;36mOptimizedCodeChecker.analyze_code\u001b[1;34m(self, code, submission_database, parallel)\u001b[0m\n\u001b[0;32m     14\u001b[0m     future_plag \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecutor\u001b[38;5;241m.\u001b[39msubmit(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplagiarism_detector\u001b[38;5;241m.\u001b[39mdetect_plagiarism, code,submission_database)\n\u001b[0;32m     16\u001b[0m     ai_result \u001b[38;5;241m=\u001b[39m future_ai\u001b[38;5;241m.\u001b[39mresult()\n\u001b[1;32m---> 17\u001b[0m     plag_result \u001b[38;5;241m=\u001b[39m \u001b[43mfuture_plag\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     19\u001b[0m     ai_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mai_detector\u001b[38;5;241m.\u001b[39mdetect_ai_generated(code)\n",
      "File \u001b[1;32me:\\project\\ML\\syntax_env\\lib\\concurrent\\futures\\_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    449\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[1;32me:\\project\\ML\\syntax_env\\lib\\concurrent\\futures\\_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 403\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    404\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    405\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    406\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32me:\\project\\ML\\syntax_env\\lib\\concurrent\\futures\\thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs)\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "Cell \u001b[1;32mIn[8], line 177\u001b[0m, in \u001b[0;36mOptimizedPlagiarismDetector.detect_plagiarism\u001b[1;34m(self, code, submission_database)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;124;03mMain plagiarism detection method\u001b[39;00m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;124;03m    submission_database: Optional list of previous submissions\u001b[39;00m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    168\u001b[0m results \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis_plagiarized\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    170\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfidence\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.0\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    174\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrisk_level\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNONE\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    175\u001b[0m }\n\u001b[1;32m--> 177\u001b[0m pattern_matches \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_pattern_database\u001b[49m(code)\n\u001b[0;32m    178\u001b[0m results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpattern_matches\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pattern_matches\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pattern_matches:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'OptimizedPlagiarismDetector' object has no attribute 'check_pattern_database'"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    checker = OptimizedCodeChecker()\n",
    "    \n",
    "    test_code = '''\n",
    "def bubble_sort(arr):\n",
    "    for i in range(len(arr)):\n",
    "        for j in range(len(arr) - i - 1):\n",
    "            if arr[j] > arr[j+1]:\n",
    "                arr[j], arr[j+1] = arr[j+1], arr[j]\n",
    "    return arr\n",
    "    '''\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"COMPREHENSIVE CODE ANALYSIS TEST\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    result = checker.analyze_code(test_code)\n",
    "    \n",
    "    print(f\"\\n\\nOVERALL ASSESSMENT:\")\n",
    "    print(f\"  Suspicious: {result['overall_suspicious']}\")\n",
    "    print(f\"  Risk Level: {result['overall_risk_level']}\")\n",
    "    print(f\"  Action: {result['action']}\")\n",
    "    print(f\"  Recommendation: {result['recommendation']}\")\n",
    "    \n",
    "    print(f\"\\nAI DETECTION:\")\n",
    "    print(f\"  Probability: {result['ai_detection']['ai_probability']:.1%}\")\n",
    "    print(f\"  Risk: {result['ai_detection']['risk_level']}\")\n",
    "    \n",
    "    print(f\"\\nPLAGIARISM DETECTION:\")\n",
    "    print(f\"  Is Plagiarized: {result['plagiarism_detection']['is_plagiarized']}\")\n",
    "    print(f\"  Confidence: {result['plagiarism_detection']['confidence']:.1%}\")\n",
    "    print(f\"  Risk: {result['plagiarism_detection']['risk_level']}\")\n",
    "    \n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76c3591",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Model Saving and Deployment\n",
    "\n",
    "This section demonstrates how to save the optimized GPT-2 model and tokenizer in HuggingFace format for future use or production deployment. Saving models in this way allows for easy loading, sharing, and integration into backend services or cloud environments.\n",
    "\n",
    "- **save_dir**: The directory where the model and tokenizer will be stored.\n",
    "- **gpt2_model.save_pretrained(save_dir)**: Saves the model weights and configuration.\n",
    "- **gpt2_tokenizer.save_pretrained(save_dir)**: Saves the tokenizer files for consistent preprocessing.\n",
    "\n",
    "> After saving, you can reload the model and tokenizer using `from_pretrained(save_dir)` in any compatible environment.\n",
    "\n",
    "**Tip:** Always version your saved models and document the training or fine-tuning process for reproducibility and auditability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d42aa24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer saved in ../models\n"
     ]
    }
   ],
   "source": [
    "# Save the optimized model and tokenizer\n",
    "save_dir = \"../models\"                 # choose a folder name\n",
    "gpt2_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# save everything in Hugging-Face format\n",
    "gpt2_model.save_pretrained(save_dir)\n",
    "gpt2_tokenizer.save_pretrained(save_dir)\n",
    "\n",
    "print(f\"Model and tokenizer saved in {save_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4e9e18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
